{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Management\n",
        "\n",
        "## Introduction\n",
        "\n",
        "### Defination of Data Management\n",
        "\n",
        "Data management is the process of collecting, storing, organizing, protecting, verifying, and processing essential data while ensuring its accessibility, reliability, and timeliness.\n",
        "\n",
        "### Importance of Data Management\n",
        "\n",
        "**Improves Efficiency**: Organized data reduces time spent searching for information.\n",
        "\n",
        "**Regulatory Compliance**: Helps meet legal and regulatory data handling requirements.\n",
        "\n",
        "**Facilitates Data Security**: Safeguards sensitive information from unauthorized access.\n",
        "\n",
        "**Cost Management**: Efficient data management can significantly reduce costs related to storage, processing, and data recovery.\n",
        "\n"
      ],
      "metadata": {
        "id": "5BqbDeU31J3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Component of Data Management\n",
        "\n",
        "1. Data Governance\n",
        "  *   Policies and procedures\n",
        "  *   Compliance, privacy, and ethics\n",
        "  *   Stewardship and ownership\n",
        "\n",
        "2. Data Architecture\n",
        "  *   Designing data systems\n",
        "  *   Integration strategies\n",
        "  *   Data modeling and metadata management\n",
        "\n",
        "3. Data Storage and Operations\n",
        "  *   Data warehousing\n",
        "  *   Database management systems (DBMS)\n",
        "  *   Data lakes and data lakehouses\n",
        "\n",
        "4. Data Security\n",
        "  *   Encryption and masking\n",
        "  *   Access controls and audit trails\n",
        "  *   Dealing with data breaches"
      ],
      "metadata": {
        "id": "OVgd7ibH37tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automation in Data Management Using AI/ML\n",
        "\n",
        "**Data Cleaning**: AI and ML algorithms can automatically detect and correct errors in datasets, such as missing values, duplicate records, outliers, and inconsistent entries.\n",
        "\n",
        "  *   Pattern Recognition: ML algorithms can learn from examples to detect patterns and anomalies within the data, allowing for the identification and remediation of errors.\n",
        "  *   Natural Language Processing (NLP): NLP can be used to clean text data, such as correcting spelling errors or standardizing terminology.\n",
        "\n",
        "**Storage Optimization**: AI/ML can aid in the efficient storage of data by identifying the most appropriate storage mechanisms and formats for different types of data.\n",
        "\n",
        "  *   Compression: ML algorithms can determine patterns and redundancies in data to compress it without loss, thus saving storage space.\n",
        "  *   Data Deduplication: AI can detect and eliminate redundant copies of data to free up storage space\n",
        "\n",
        "**Ensuring Data Quality**: ML models can continuously learn and adapt to ensure data remains high quality over time, flagging and correcting new types of errors as they emerge.\n",
        "\n",
        "  *   Anomaly Detection: AI algorithms can continuously monitor data streams to detect outliers or unusual patterns that may indicate errors or fraud.\n",
        "  *   Monitoring Changes: AI systems can track changes over time to ensure that the data quality is not degrading and that it aligns with compliance standards."
      ],
      "metadata": {
        "id": "UN49fiT-9Efl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "MtoHRm7g-T2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn matplotlib seaborn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs"
      ],
      "metadata": {
        "id": "ayq-hZI8-S2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "We create a synthetic dataset using sklearn's `make_blobs` functions"
      ],
      "metadata": {
        "id": "_qzFEO33C7TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, _ = make_blobs(n_samples=1000, centers=3, n_features=3, random_state=42)\n",
        "df = pd.DataFrame(data, columns=['Feature_1', 'Feature_2', 'Feature_3'])\n",
        "print(df.shape)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "p1Y3URoW-qak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the raw data\n",
        "Generate a scatter plot to visualize the raw data points."
      ],
      "metadata": {
        "id": "6k7dXas8DNL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "plt.scatter(df['Feature_1'], df['Feature_2'], df['Feature_3'])\n",
        "plt.title('Visualization of Raw Data')\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_zlabel('Feature 3')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nybglGk7-seN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning\n",
        "In this step, we perform the automated data cleaning techniques, like removing duplicates, handling missing values, or outlier detection."
      ],
      "metadata": {
        "id": "qJQqprJhDSNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E.g., Removing duplicates\n",
        "df = df.drop_duplicates()\n",
        "df"
      ],
      "metadata": {
        "id": "Z8Tb3NjF-xUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Transformation\n",
        "Applying Principal Component Analysis (PCA) to reduce dimensions."
      ],
      "metadata": {
        "id": "sNjfwQ1oDqDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df)\n",
        "\n",
        "plt.scatter(df_pca[:, 0], df_pca[:, 1])\n",
        "plt.title('Visualization of PCA Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YLAg22vx-zr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pca"
      ],
      "metadata": {
        "id": "AheMi3w0DZUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automated Data Classification\n",
        "### Data preparation\n",
        "Split the dataset into training and test sets."
      ],
      "metadata": {
        "id": "FR716Z_4D5Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_pca, _, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HMLPJwKL-4J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "Train a simple machine learning model to classify the data points."
      ],
      "metadata": {
        "id": "9PLxKa3LD_zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qU4B9LhYCcQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "Evaluate model performance with appropriate metrics and visualize the results."
      ],
      "metadata": {
        "id": "6jK8AGeYEIc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Z_4PCDxnCiht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a automated pipeline\n",
        "\n",
        "Create an end-to-end ML pipeline for data preprocessing, cleaning, and classification."
      ],
      "metadata": {
        "id": "aa0LPE2EEQH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=2)),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "uzO8FPXpCpad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXdlbUechfNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Algorithms (Practice)\n",
        "\n",
        "We can apply KMeans(https://en.wikipedia.org/wiki/K-means_clustering) and DBSCAN(https://en.wikipedia.org/wiki/DBSCAN) to do the same job"
      ],
      "metadata": {
        "id": "Gv5cdSN0Ii1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Generate the dataset\n",
        "X, y = make_blobs(n_samples=1000, centers=3, n_features=3, random_state=42)\n",
        "\n",
        "# Apply KMeans\n",
        "\n",
        "# Apply DBSCAN\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "VbgpbANHJ1pd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}